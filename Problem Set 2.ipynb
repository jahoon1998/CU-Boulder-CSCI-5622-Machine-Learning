{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54d67933",
   "metadata": {},
   "source": [
    "# Problem Set 2 - Decision Trees, Ensemble Methods\n",
    "***\n",
    "**Name**:Jahoon Koo\n",
    "***\n",
    "\n",
    "This assignment is due on Canvas by **11:59PM on Wednesday February 22**.\n",
    "\n",
    "Submit only this Jupyter notebook to Canvas with the name format `PS2_<yourname>.ipynb`. Do not compress it using tar, rar, zip, etc.\n",
    "Your solutions to analysis questions should be done in Markdown directly below the associated question.\n",
    "\n",
    "Remember that you are encouraged to discuss the problems with your classmates and instructors, \n",
    "but **you must write all code and solutions on your own**, and list any people or sources consulted.\n",
    "The only exception to this rule is that you may copy code directly from your own solution to homework 1.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b19ba6a",
   "metadata": {},
   "source": [
    "## Overview \n",
    "\n",
    "Your task for this homework is to build a decision tree classifier from scratch. Of course, we provide some initial classes\n",
    "that you'll be editing. Since last two problems will use the scikit-learn's DecisionTreeClassifier, your solution\n",
    "does not have to be efficient as long as it passes the sanity checks in a reasonable time (typically less than ~1min).\n",
    "\n",
    "We will run a small comparison between our implementation and Scikit's in Problem 2 to make sure we didn't miss anything.\n",
    "\n",
    "The third part will introduce k-fold cross validation to find out how deep is the best decision tree classifier. The last problem\n",
    "requires a _weak learner_ (implemented as `base` model), so we'll use a decision tree that yields lower performance. But with _Ensemble Methods_,\n",
    "we will be able to improve the performance by aggregating predictions from multiple weak learners.\n",
    "For the ensemble methods, we'll explore bagging, Random Forest, and boosting (AdaBoost).\n",
    "\n",
    "Any Machine Learning interview will almost certainly have a question or two about decision trees and how they're trained.\n",
    "So understanding the code and trying to implement everything on your own will be the best way to prepare for such interviews.\n",
    "\n",
    "Also remember, if your code is correct then the sanity checks should pass without any major issue.\n",
    "But if the sanity checks pass that does not necessarily imply your code is 100% correct.\n",
    "\n",
    "Happy coding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a79c515",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import tests\n",
    "import data\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78586b46",
   "metadata": {},
   "source": [
    "### Problem 1 - Decision Trees [30 points]\n",
    "***\n",
    "The goal of this problem is to implement the core elements of the Decision Tree classifier.\n",
    "We do not expect a highly efficient implementation of the functions since the ensemble methods will use the implementation from scikit-learn.\n",
    "\n",
    "\n",
    "|Age|Salary|Colorado Resident| Has Siblings | College degree|\n",
    "|:------:|:-----------:| :----------:| :----------:|--:|\n",
    "| 37 | 44,000 | Yes | No  | Yes|\n",
    "| 61 | 52,000 | Yes | No  | No |\n",
    "| 23 | 44,000 | No  | No  | Yes|\n",
    "| 39 | 38,000 | No  | Yes | Yes|\n",
    "| 48 | 49,000 | No  | No  | Yes|\n",
    "| 57 | 92,000 | No  | Yes | No |\n",
    "| 38 | 41,000 | No  | Yes | Yes|\n",
    "| 27 | 35,000 | Yes | No  | No |\n",
    "| 23 | 26,000 | Yes | No  | No |\n",
    "| 38 | 45,000 | No  | No  | No |\n",
    "| 32 | 50,000 | No  | No  | Yes|\n",
    "| 25 | 52,000 | Yes | No  | Yes|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fea8fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.array([\n",
    "    [37, 44000, 1, 0],\n",
    "    [61, 52000, 1, 0],\n",
    "    [23, 44000, 0, 0],\n",
    "    [39, 38000, 0, 1],\n",
    "    [48, 49000, 0, 0],\n",
    "    [57, 92000, 0, 1],\n",
    "    [38, 41000, 0, 1],\n",
    "    [27, 35000, 1, 0],\n",
    "    [23, 26000, 1, 0],\n",
    "    [38, 45000, 0, 0],\n",
    "    [32, 50000, 0, 0],\n",
    "    [25, 52000, 1, 0]\n",
    "])\n",
    "labels = np.array([1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2708ac",
   "metadata": {},
   "source": [
    "Each leaf node (terminal node) in a decision tree has a label value assigned to it. The same label will be assigned\n",
    "to all samples that reach the leaf node.\n",
    "- 1.1 [2 pts] What is the best accuracy for a baseline classifier that predicts one label for all rows on the dataset above?\n",
    "which label should it predict?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6688d12d",
   "metadata": {},
   "source": [
    "% Write-up for 1.1 <br>\n",
    "#BEGIN <br>\n",
    "The best accuracy for a baseline classifier is 58.3% when it predicts all rows on the dataset above 1 because there are seven 1s out of 12 labels.\n",
    "\n",
    "#END<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ab7f5e",
   "metadata": {},
   "source": [
    "- 1.2 [3 pts] Complete `compute_label` to return the label that should be assigned to the leaf node based on training labels in `y`.\n",
    "\n",
    "If more than one label are possible, choose the one with the lowest value (e.g, if both `0` and `1` are possible,\n",
    "choose `0`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56f33221",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    \"\"\"Base class for LeafNode and ParentNode\"\"\"\n",
    "    left_child = None\n",
    "    right_child = None\n",
    "    def feature_importance(self, importance_dict):\n",
    "        return importance_dict\n",
    "\n",
    "class LeafNode(Node):\n",
    "    def __init__(self, y):\n",
    "        \"\"\"\n",
    "        :param y: 1-d array containing labels, of shape (num_points,)\n",
    "        \"\"\"\n",
    "        self.label = self.compute_label(y)\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_label(y):\n",
    "        \"\"\"\n",
    "        return the label that yields best performance if predicted of all instances in y\n",
    "        :param y:  1-d array containing labels\n",
    "        :return: single label, integer\n",
    "        \"\"\"\n",
    "        node_label = None\n",
    "        #Workspace 1.2\n",
    "        #TODO: Return the label that should be assigned to the leaf node\n",
    "        #In case of multiple possible labels, choose the one with the lowest value\n",
    "        #Make no assumptions about the number of class labels\n",
    "        #BEGIN\n",
    "        \n",
    "        unique_labels, label_counts = np.unique(y, return_counts=True)\n",
    "        #np.argmax returns the index of the lowest value in case of multiple possible labels\n",
    "        node_label = unique_labels[np.argmax(label_counts)]\n",
    "        \n",
    "        #END\n",
    "        return node_label\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        return the label for one obervation x\n",
    "        :param x: one sample, of shape (num_features)\n",
    "        :return: label, integer\n",
    "        \"\"\"\n",
    "        return self.label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84bcaed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1.2: [PASS]\n"
     ]
    }
   ],
   "source": [
    "# Test cell, uncomment to run the tests\n",
    "tests.test_leaf(LeafNode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ce396b",
   "metadata": {},
   "source": [
    "The tree also contains _parent nodes_. They can either be parents of: leaf nodes, parent nodes, or a combination of the two.\n",
    "Each parent node has a left and a right child. A parent node is used when we can reduce the impurity of the labels by splitting\n",
    "the training instances based on a certain threshold.\n",
    "\n",
    "First, we'll need to choose an impurity measure. For classification,\n",
    "there are two mainstream measures: _gini index_ and _entropy_. We'll be using the former for our implementation.\n",
    "\n",
    "\\begin{align}\n",
    "\\text{Gini}(y) = 1 - \\sum_{c}  (p_c)^2 \\text{  and  Entropy}(y) = -\\sum_{c}  p_c . \\log p_c ,\n",
    "\\end{align}\n",
    "\n",
    "where $p_c$ is the probability of occurrence (ratio)  of class $c$ among the labels in $y$. *Make sure that the log function being used for entropy is np.log() as it calculates $log_e()$*  \n",
    "\n",
    "- 1.3 [3 pts] Complete the function `gini` that returns the gini index of labels in `y`.\n",
    "\n",
    "_Hint: Make sure you handle multi-class labels\n",
    "(not just binary)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02dd7e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(y):\n",
    "    \"\"\"\n",
    "    :param y: 1-d array contains labels, of shape (num_points,)\n",
    "    :return: float, entropy measure of the labels\n",
    "    \"\"\"\n",
    "    gini_index = 0\n",
    "    # Workspace 1.3\n",
    "    #TODO: Compute the gini index of the labels\n",
    "    #BEGIN\n",
    "    \n",
    "    unique_labels, label_counts = np.unique(y, return_counts=True)\n",
    "    label_probability = {}\n",
    "    number_of_labels = len(y)\n",
    "    for label, count in zip(unique_labels, label_counts):\n",
    "        label_probability[label] = count / number_of_labels\n",
    "\n",
    "    for label in label_probability:\n",
    "        gini_index += (label_probability[label] ** 2)\n",
    "    \n",
    "    #END\n",
    "    gini_index = 1 - gini_index\n",
    "    return gini_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7aaca0a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1.3: [PASS]\n"
     ]
    }
   ],
   "source": [
    "# Test cell, uncomment to run the tests\n",
    "tests.test_gini(gini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ab1108",
   "metadata": {},
   "source": [
    "Now that we're at a parent node, we decide to partition our label instances in $S$ to two parts indexed by $P_1$ and $P_2$,\n",
    "and we want to compute how much this split reduces the impurity.\n",
    "\n",
    "Using the impurity measure $\\mathcal{M}$, this impurity reduction is computed as follows:\n",
    "\\begin{align}\n",
    "\\text{Reduction}(S, {P_1, P_2}) = \\mathcal{M}(S) - \\big[\n",
    "    \\frac{|P_1|}{|S|} .\\mathcal{M}(S[P_1]) + \\frac{|P_2|}{|S|}.\\mathcal{M}(S[P_2])\n",
    "    \\big],\n",
    "\\end{align}\n",
    "\n",
    "where $|A|$ denotes the size of the set $A$.\n",
    "\n",
    "The main questions will be based on the entropy measure, in which case the `Reduction` is also called _information gain_\n",
    "(reducing the entropy implies that the partitioning decision variable and the labels have a higher mutual information).\n",
    "\n",
    "-  1.4 [3 pts] Complete the `impurity_reduction` function to return the impurity reduction of the split using the provided measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45773a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impurity_reduction(y, left_indices, right_indices, impurity_measure=gini):\n",
    "    \"\"\"\n",
    "    :param y: all labels\n",
    "    :param left_indices: the indices of the elements of y that belong to the left child\n",
    "    :param right_indices: the indices of the elements of y that belong to the right child\n",
    "    :param impurity_measure: function that takes 1d-array of labels and returns the impurity measure, defaults to gini\n",
    "    :return: impurity reduction of the split\n",
    "    \"\"\"\n",
    "    impurity_reduce = 0\n",
    "    # Workspace 1.4\n",
    "    #BEGIN\n",
    "    impurity_y = impurity_measure(y)\n",
    "    impurty_left = impurity_measure([y[index] for index in left_indices])\n",
    "    impurty_right = impurity_measure([y[index] for index in right_indices])\n",
    "    \n",
    "    impurity_reduce = impurity_y - (((len(left_indices)/len(y)) * impurty_left) + ((len(right_indices)/len(y)) * impurty_right))\n",
    "    #END\n",
    "    return impurity_reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b17ba29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1.4: [PASS]\n"
     ]
    }
   ],
   "source": [
    "# Test cell, uncomment to run the tests\n",
    "tests.test_information_gain(impurity_reduction, gini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c4dadd",
   "metadata": {},
   "source": [
    "We'll use `best_partition` to look up for the feature and threshold that yields the partition with the best impurity reduction.\n",
    "\n",
    "For each feature:\n",
    " - Compute all possible thresholds (use `split_values`)\n",
    " - For each threshold:\n",
    "    - Split to `(left_indices, right_indices)` based on the threshold\n",
    "    - Compute the impurity reduction of the split\n",
    "\n",
    "The function then returns the feature and the threshold that yield the best impurity reduction (and the reduction value)\n",
    "\n",
    " - 1.5 [5 pts] Complete `best_partition`.\n",
    " \n",
    " _Hint: `split_values` is provided as a helper function. It takes the feature column and returns\n",
    "the set of thresholds_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "115b4d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_values(feature_values):\n",
    "    \"\"\"\n",
    "    Helper function to return the split values. if feature consists of the values f1 < f2 < f3 then\n",
    "    this returns [(f2 + f1)/2, (f3 + f2)/2]\n",
    "    :param feature_values: 1-d array of shape (num_points)\n",
    "    :return: array of shape (max(m-1, 1),) where m is the number of unique values in feature_values\n",
    "    \"\"\"\n",
    "    unique_values = np.unique(feature_values)\n",
    "    if unique_values.shape[0] == 1:\n",
    "        return unique_values\n",
    "    return (unique_values[1:] + unique_values[:-1]) / 2\n",
    "\n",
    "\n",
    "def best_partition(X, y, impurity_measure=gini):\n",
    "    \"\"\"\n",
    "    :param X: features array, shape (num_samples, num_features)\n",
    "    :param y: labels of instances in X, shape (num_samples)\n",
    "    :param impurity_measure: function that takes 1d-array of labels and returns the impurity measure\n",
    "    :return: Return the best value and its corresponding threshold by splitting based on the different features.\n",
    "    \"\"\"\n",
    "\n",
    "    best_feature, best_threshold, best_reduction = 0, 0, -np.inf\n",
    "\n",
    "    #Workspace 1.5\n",
    "    #TODO: Complete the function as detailed in the question and return description\n",
    "    #BEGIN\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    for feature in range(n_features):\n",
    "        feature_values = X[:, feature]\n",
    "        thresholds = split_values(feature_values)\n",
    "        for threshold in thresholds:\n",
    "            left_indices = np.flatnonzero(X[:, feature] < threshold)\n",
    "            right_indicies = np.flatnonzero(X[:, feature] >= threshold)\n",
    "            reduction = impurity_reduction(y, left_indices, right_indicies, impurity_measure)\n",
    "            if reduction > best_reduction:\n",
    "                best_feature = feature\n",
    "                best_threshold = threshold\n",
    "                best_reduction = reduction\n",
    "    #END\n",
    "    return best_feature, best_threshold, best_reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e88a62e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1.5: [PASS]\n"
     ]
    }
   ],
   "source": [
    "# Test cell, uncomment to run the tests\n",
    "# If you chose to not use split_values, then this test will likely fail\n",
    "tests.test_best_partition(best_partition, gini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d515d5f",
   "metadata": {},
   "source": [
    "We provide the implementation of the parent node below. Note that the `left_child` will take instance for which\n",
    "`feature_id` value is < `feature_threshold`. We should construct our decision tree as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d1c2e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParentNode:\n",
    "\n",
    "    def __init__(self, feature_id, feature_threshold, left_child: Node, right_child: Node, weighted_impurity=0):\n",
    "        \"\"\"\n",
    "        Initialize a parent node.\n",
    "        :param feature_id: the feature index on which the splitting will be done\n",
    "        :param feature_threshold: the feature threshold. Left child takes item with features[features_id] < threshold\n",
    "        :param left_child: left child node\n",
    "        :param right_child: right child node\n",
    "        :param weighted_impurity: weighted impurity reduction, optional (used for the bonus question)\n",
    "        \"\"\"\n",
    "        self.feature_id = feature_id\n",
    "        self.threshold = feature_threshold\n",
    "        self.left_child = left_child\n",
    "        self.right_child = right_child\n",
    "        self.weighted_impurity = weighted_impurity\n",
    "\n",
    "    def feature_importance(self, importance_dict):\n",
    "        \"\"\"\n",
    "        :param importance_dict: dictionary, keys are features indices and value are feature importances\n",
    "        :return: updated feature importrances dictionary\n",
    "        \"\"\"\n",
    "        #Workspace 2.5.a (bonus)\n",
    "        #BEGIN\n",
    "        \n",
    "        #END\n",
    "        return importance_dict\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Predict the label of row x. If we're a leaf node, return the value of the leaf. Otherwise, call predict\n",
    "        of the left/right child (depending on x[feature_index).\n",
    "        This will be called by DecisionTree.predict\n",
    "        :param x: 1-d array of shape (num_features)\n",
    "        :return: integer, the label for x\n",
    "        \"\"\"\n",
    "        if x[self.feature_id] < self.threshold:\n",
    "            label = self.left_child.predict(x)\n",
    "        else:\n",
    "            label = self.right_child.predict(x)\n",
    "        return label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8998d641",
   "metadata": {},
   "source": [
    "Now we tackle the core of a decision tree. The tree is built in a recursive way. The recursion in `DecisionTree.build` works as follows:\n",
    "- Parameters: `min_samples_split`, `impurity_measure`\n",
    "- Inputs: `features`, `labels`, `depth`\n",
    "- Base case of the recursion, return a leaf node if either:\n",
    "    - `depth` is 0\n",
    "    - `labels` contains less than `min_samples_split` elements\n",
    "    - There is no impurity reduction (reduction<=0 for all splits)\n",
    "- Recursion (there is a split with impurity reduction > 0):\n",
    "    - create the left and right child nodes with `depth - 1`\n",
    "    - return the parent node\n",
    "\n",
    "The left child node will contain instances for which the feature with index `best_feature` is strictly lower than\n",
    "`best_threshold` of the partition. The right child takes the remaining instances.\n",
    "\n",
    "- 1.6 [6 pts] Complete `build` method of `DecisionTree`\n",
    "- 1.7 [2 pts] Complete the `score` method that returns the accuracy on the given data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a45c1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "\n",
    "    def __init__(self, max_depth=-1, min_samples_split=2, impurity_measure=gini):\n",
    "        \"\"\"\n",
    "        Initialize the decision tree\n",
    "        :param max_depth: maximum depth of the tree\n",
    "        :param min_samples_split: minimum number of samples required for a split\n",
    "        :param impurity_measure: impurity measure function to use for best_partition, default to entropy\n",
    "        \"\"\"\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.impurity_measure = impurity_measure\n",
    "        self.root = None\n",
    "        self.num_features = None\n",
    "\n",
    "    def build(self, X, y, depth) -> Node:\n",
    "        \"\"\"\n",
    "        Recursive method used to build the decision tree nodes\n",
    "        :param X: data that are used to build the tree, of shape (num_samples, num_features)\n",
    "        :param y: labels of the samples in features, of shape (num_samples)\n",
    "        :param depth: depth of the tree to create\n",
    "        :return: the root node of the tree\n",
    "        \"\"\"\n",
    "        # Workspace 1.6\n",
    "        #BEGIN\n",
    "        n_samples, n_features = X.shape \n",
    "        \n",
    "        feature_id, feature_threshold, best_reduction = best_partition(X, y, self.impurity_measure)    \n",
    "        if depth == 0 or n_samples < self.min_samples_split or best_reduction <= 0:\n",
    "            return LeafNode(y)\n",
    "        else:\n",
    "            \n",
    "            left_indices = np.flatnonzero(X[:, feature_id] < feature_threshold)\n",
    "            right_indicies = np.flatnonzero(X[:, feature_id] >= feature_threshold)\n",
    "\n",
    "            left_child = self.build(X[left_indices], y[left_indices], depth - 1)\n",
    "            right_child = self.build(X[right_indicies], y[right_indicies], depth - 1)\n",
    "            \n",
    "            return ParentNode(feature_id, feature_threshold, left_child, right_child)\n",
    "                        \n",
    "        \n",
    "        #END\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        :param X: Training samples\n",
    "        :param y: training labels\n",
    "        :return: trained classifier\n",
    "        \"\"\"\n",
    "        self.num_features = X.shape[1]\n",
    "        self.root = self.build(X, y, self.max_depth)\n",
    "        return self\n",
    "\n",
    "    def compute_importance(self, features_names=None):\n",
    "        \"\"\"\n",
    "        Compute the normalized feature importances\n",
    "        :param features_names: Name of features to use, defaults to integers\n",
    "        :return: Dictionary with feature_name: feature_importance\n",
    "        \"\"\"\n",
    "        if features_names is None:\n",
    "            features_names = [\"feat_%i\" % i for i in range(self.num_features)]\n",
    "        feats_importances = {i:0.0 for i in range(self.num_features)} # to include\n",
    "        # Workspace 2.5.b (bonus)\n",
    "        # ToDo: Call the root's feature and importance and scale values in feats_importance to sum to 1\n",
    "        total_importances = 1\n",
    "        #BEGIN\n",
    "        \n",
    "        #END\n",
    "        return {features_names[k] :v for k,v in feats_importances.items() if v>0}\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Loops through rows of X and predicts the labels one row at a time\n",
    "        \"\"\"\n",
    "        y_hat = np.zeros((X.shape[0],), int)\n",
    "        for i in range(X.shape[0]):\n",
    "            y_hat[i] = self.root.predict(X[i])\n",
    "        return y_hat\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Return the mean accuracy on the given test data and labels.\n",
    "        :param X: Test samples, shape (num_points, num_features)\n",
    "        :param y: true labels for X, shape (num_points,)\n",
    "        :return: mean accuracy\n",
    "        \"\"\"\n",
    "        accuracy = 0\n",
    "        # Workspace 1.7\n",
    "        #BEGIN\n",
    "        y_hat = self.predict(X)\n",
    "        for i in range(len(y)):\n",
    "            if y_hat[i] == y[i]:\n",
    "                accuracy += 1\n",
    "        \n",
    "        accuracy /= len(y)\n",
    "        #END\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5110b85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1.6: [PASS]\n"
     ]
    }
   ],
   "source": [
    "# Test cell, uncomment to run the tests\n",
    "# If you chose to not use split_values, then this test will likely fail\n",
    "tests.test_tree_build(DecisionTree, gini)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27de221c",
   "metadata": {},
   "source": [
    "- 1.8 [2 pts] We want to evaluate our `DecisionTree(max_depth=3, min_samples_split=2`.\n",
    "What's the accuracy we achieve on the training data using the tree? ( we train and evaluate using `(features, labels)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a9c03a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9166666666666666\n"
     ]
    }
   ],
   "source": [
    "# Workspace 1.8\n",
    "#BEGIN\n",
    "decision_tree = DecisionTree(max_depth=3, min_samples_split=2).fit(features, labels)\n",
    "accuracy = decision_tree.score(features, labels)\n",
    "print(accuracy)\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afef1ee7",
   "metadata": {},
   "source": [
    "- 1.9 [2 pts] Using `min_samples_split=2`, what is the minimum depth so that our `DecisionTree` fits perfectly our\n",
    "training data `(labels, features)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b3f30066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "depth: 1, accuracy: 0.75\n",
      "depth: 2, accuracy: 0.9166666666666666\n",
      "depth: 3, accuracy: 0.9166666666666666\n",
      "depth: 4, accuracy: 0.9166666666666666\n",
      "depth: 5, accuracy: 1.0\n",
      "minimum depth is 5.\n"
     ]
    }
   ],
   "source": [
    "# Workspace 1.9\n",
    "# To show that the minimum required depth is n, you can provide the accuracy for depth = (n-1) and depth = n\n",
    "#BEGIN\n",
    "min_depth = -1\n",
    "for depth in range(1,10):\n",
    "    decision_tree = DecisionTree(max_depth=depth, min_samples_split=2).fit(features, labels)\n",
    "    accuracy = decision_tree.score(features, labels)\n",
    "    print(\"depth: {}, accuracy: {}\".format(str(depth), str(accuracy)))\n",
    "    if accuracy == 1:\n",
    "        min_depth = depth\n",
    "        break\n",
    "\n",
    "print(\"minimum depth is {}.\".format(str(min_depth)))\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86af7ea",
   "metadata": {},
   "source": [
    "We provide an example below to display the structure of a decision tree. Look at print_tree() in tests.\\_\\_init\\_\\_.py to understand how this visualization is working.\n",
    "- 1.10 (2pts) Edit it to show the tree for the required minimum depth found in 1.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04c048ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  ┌│label: 0\n",
      "       ┌|salary  │┘\n",
      "       │|36500.00│┐\n",
      "       │          │       ┌│label: 1\n",
      "       │          └|age  │┘\n",
      "       │           |37.50│┐\n",
      "       │                  │                  ┌│label: 1\n",
      "       │                  │       ┌|salary  │┘\n",
      "       │                  │       │|43000.00│┐\n",
      "       │                  │       │          └│label: 0\n",
      "       │                  └|age  │┘\n",
      "       │                   |38.50│┐\n",
      "       │                          └│label: 1\n",
      "|age  │┘\n",
      "|52.50│┐\n",
      "       └│label: 0\n"
     ]
    }
   ],
   "source": [
    "tree = DecisionTree(max_depth=5, min_samples_split=2).fit(features, labels)\n",
    "tests.print_tree(tree, [\"age\", \"salary\", \"resident\", \"siblings\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8f63d1",
   "metadata": {},
   "source": [
    "### Problem 2: DecisionTree vs DecisionTreeClassifier [6 points]\n",
    "\n",
    "We've just showed that our decision tree is better than the naive NaiveBayes! Let see how it compares to scikit's\n",
    "DecisionTreeClassifier.\n",
    "\n",
    "First, we'll need a fancier dataset. We are going to predict the level of usage of a bike sharing system in Washington, DC using the decision trees.\n",
    "\n",
    "We start by loading preprocessed data that we'll use. Since the original Bike Sharing\n",
    " [dataset](https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset)\n",
    " is for regression, we have to transform `BikeSharing.y_train` and `BikeSharing.y_test` to discrete values reflecting the level of usage.\n",
    "We have included this dataset with the homework -- you can find it in the data directory.\n",
    "\n",
    "|Bike Rentals| Label|\n",
    "|:----------:|--:|\n",
    "| $ P < $2000|0|\n",
    "|2000$\\leq P < $ 4000| 1 |\n",
    "|4000$ \\leq P < $ 6000| 2 |\n",
    "|6000$ \\leq P $ | 3 |\n",
    "\n",
    "- 2.1 [3 pts] Start by transforming `y_train` and `y_test` of `bike_sharing` to discrete values using the provided ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0eef24c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1. 2. 3.] (584, 12)\n",
      "[0. 1. 2. 3.] (147, 12)\n"
     ]
    }
   ],
   "source": [
    "bike_sharing = data.BikeSharing()\n",
    "#Workspace 2.1\n",
    "#TODO: Discretize y_train and y_test\n",
    "#BEGIN\n",
    "def regression_data_to_discrete_data(y):\n",
    "    for i in range(len(y)):\n",
    "        if y[i] < 2000:\n",
    "            y[i] = 0\n",
    "        elif y[i] >= 2000 and y[i] < 4000:\n",
    "            y[i] = 1\n",
    "        elif y[i] >= 4000 and y[i] < 6000:\n",
    "            y[i] = 2\n",
    "        elif y[i] >= 6000:\n",
    "            y[i] = 3\n",
    "\n",
    "regression_data_to_discrete_data(bike_sharing.y_train)       \n",
    "regression_data_to_discrete_data(bike_sharing.y_test)         \n",
    "#END\n",
    "print(np.unique(bike_sharing.y_train), bike_sharing.X_train.shape)\n",
    "print(np.unique(bike_sharing.y_test), bike_sharing.X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a442f677",
   "metadata": {},
   "source": [
    "- 2.2 [3 pts] Compare our `DecisionTree` and scikit's `DecisionTreeClassifier` on the bike sharing dataset by reporting the accuracies on the test data.\n",
    "\n",
    " [scikit's `DecisionTreeClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)\n",
    "uses Gini Index by default and shuffles the features before each split. Refer to the documentation for more information about how to change the impurity measure if you are curious.\n",
    "\n",
    "Use `max_depth = 5, min_samples_split=2, random_state=11` for the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3478446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of our Decision Tree on the test data is 0.7074829931972789.\n",
      "The accuracy of scikit's DecisionTreeClassifier on the test data is 0.7074829931972789.\n"
     ]
    }
   ],
   "source": [
    "# Workspace 2.2.a\n",
    "#BEGIN\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "our_tree = DecisionTree(max_depth=5, min_samples_split=2).fit(bike_sharing.X_train, bike_sharing.y_train)\n",
    "our_tree_accuracy = our_tree.score(bike_sharing.X_test, bike_sharing.y_test)\n",
    "print(\"The accuracy of our Decision Tree on the test data is {}.\".format(str(our_tree_accuracy)))\n",
    "\n",
    "their_tree = DecisionTreeClassifier(max_depth = 5, min_samples_split=2, random_state=11).fit(bike_sharing.X_train, bike_sharing.y_train)\n",
    "their_tree_accuracy = their_tree.score(bike_sharing.X_test, bike_sharing.y_test)\n",
    "print(\"The accuracy of scikit's DecisionTreeClassifier on the test data is {}.\".format(str(their_tree_accuracy)))\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8c1771",
   "metadata": {},
   "source": [
    "% Write-up for 2.2.b <br>\n",
    "#BEGIN <br>\n",
    "The accuracy of our Decision Tree on the test data is 0.7074829931972789.\n",
    "The accuracy of scikit's DecisionTreeClassifier on the test data is 0.7074829931972789.\n",
    "\n",
    "They both have the same accuracy when max_depth=5, min_samples_split=2, and random_state=11.\n",
    "\n",
    "#END<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf3eab2",
   "metadata": {},
   "source": [
    "### Bonus questions\n",
    "We've implemented `DecisionTree` to handle different measures of impurity. We want now to compare our implementation\n",
    "to the standard `DecisionTreeClassifier` using Gini index.\n",
    "- **(Bonus)** 2.3  [2 pts] Complete `entropy` function\n",
    "_hint: for the log function, use `np.log` and the convention `0 * log(0) = 0`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4463621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(y):\n",
    "    \"\"\"\n",
    "    :param y: 1-d array contains labels, of shape (num_points,)\n",
    "    :return: float, gini index the labels\n",
    "    \"\"\"\n",
    "    entropy_value = 0\n",
    "    # Workspace 2.3\n",
    "    #TODO: Compute the gini index of the labels in y\n",
    "    #BEGIN\n",
    "    unique_labels, label_counts = np.unique(y, return_counts=True)\n",
    "    label_probability = {}\n",
    "    number_of_labels = len(y)\n",
    "    for label, count in zip(unique_labels, label_counts):\n",
    "        label_probability[label] = count / number_of_labels\n",
    "\n",
    "    for label in label_probability:\n",
    "        if label_probability[label] == 0:\n",
    "            entropy_value += 0\n",
    "        else:\n",
    "            entropy_value += (label_probability[label] * np.log(label_probability[label]))\n",
    "    entropy_value *= -1\n",
    "    #END\n",
    "    return entropy_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0e489f",
   "metadata": {},
   "source": [
    "- **(Bonus)** 2.4 [2 pts] Perform the same comparison as in 2.2 with entropy but without setting the random state.\n",
    "How do you explain the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8bede95c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of our Decision Tree on the test data is 0.7142857142857143.\n",
      "The accuracy of scikit's DecisionTreeClassifier on the test data is 0.7074829931972789.\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2) # to fix the randomness in DecisionTreeClassifier\n",
    "# Workspace 2.4.a\n",
    "#BEGIN\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "our_tree = DecisionTree(max_depth=5, min_samples_split=2, impurity_measure=entropy).fit(bike_sharing.X_train, bike_sharing.y_train)\n",
    "our_tree_accuracy = our_tree.score(bike_sharing.X_test, bike_sharing.y_test)\n",
    "print(\"The accuracy of our Decision Tree on the test data is {}.\".format(str(our_tree_accuracy)))\n",
    "\n",
    "their_tree = DecisionTreeClassifier(max_depth = 5, min_samples_split=2, criterion=\"entropy\").fit(bike_sharing.X_train, bike_sharing.y_train)\n",
    "their_tree_accuracy = their_tree.score(bike_sharing.X_test, bike_sharing.y_test)\n",
    "print(\"The accuracy of scikit's DecisionTreeClassifier on the test data is {}.\".format(str(their_tree_accuracy)))\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcc7efe",
   "metadata": {},
   "source": [
    "% Write-up for 2.4.b <br>\n",
    "#BEGIN <br>\n",
    "The accuracy of our Decision Tree on the test data is 0.7142857142857143.\n",
    "The accuracy of scikit's DecisionTreeClassifier on the test data is 0.7074829931972789.\n",
    "\n",
    "Our decision tree performs slightly better than scikit's DecisionTreeClassifier when using entropy as impurity measure.\n",
    "\n",
    "#END<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8540fc0",
   "metadata": {},
   "source": [
    "**(Bonus)**\n",
    "\n",
    "Now we can be a bit more ambitious and compute the importance of each feature in our decision tree. The importance of feature $f$\n",
    "is the sum of the weighted impurity reduction of parent nodes that are split based on the feature $f$.\n",
    "\n",
    "The weighted impurity reduction of $node_i$ is the following:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{N_{\\text{node}_i}}{N_\\text{total}} \\times \\text{impurity reduction}({\\text{node}_i}),\n",
    "\\end{align}\n",
    "\n",
    "where $N$ is the total number of training samples, and $N_{\\text{node}_i}$ is the number of training samples that at $node_i$.\n",
    "\n",
    "Since we scale the feature importances in `DecisionTree` to sum to 1, we don't have to divide by $N_\\text{total}$\n",
    "and we can simply use:\n",
    "\n",
    "\\begin{align}\n",
    "\\text{weighted impurity}(\\text{node}_i) = N_{\\text{node}_i} \\times \\text{impurity reduction}({\\text{node}_i}),\n",
    "\\end{align}\n",
    "\n",
    "Practically, we use a dictionary `feats_importances` that maps feature indices to their importances.\n",
    "- Start with `feats_importance[f]=0` for all `f`\n",
    "- Start the recursion from the root node:\n",
    "    - Current node is split based on feature `i`\n",
    "    - add weighted impurity reduction to `feature_importance[i]`\n",
    "    - ask right and left child to do the same\n",
    "- Scale the values in `feats_importance` to sum to 1   \n",
    "- return `feats_importance`\n",
    "\n",
    "You can provide `weighted_impurity` directly when initializing the parent nodes in `DecisionTree.build`.\n",
    "\n",
    "- **(Bonus)** 2.5 [4 pts] Complete `ParentNode`'s `feature_importance`, `DecisionTree`'s `compute_importance`, and \n",
    "compare our implementation to that of scikits on bike sharing data.\n",
    "\n",
    "Use `random_state=0, splitter=\"best\"` for scikit and `max_depth=3`, `min_samples_split=2`, gini index for both.\n",
    "Note that scikit's DecisonTreeClassifier always uses Gini for the feature importance computation (even if `criterion` is set to Entropy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0dd6e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Workspace 2.5\n",
    "# Compare feature importances of DecisionTree(gini) to DecisionTreeClassifier\n",
    "# Exclude features with 0 importance from both\n",
    "#BEGIN\n",
    "\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9d2f5d",
   "metadata": {},
   "source": [
    "### Problem 3 - Model Selection via Cross Validation [16 points]\n",
    "***\n",
    "In this problem, we will be working with scikit-learn `DecisionTreeClassifier`. We want to figure out the best `max_depth`\n",
    " for our dataset.\n",
    "\n",
    "In the bike sharing dataset, we only have a training set and a test set. The question then is how do we perform the model\n",
    " selection seen in Problem Set 1?\n",
    "\n",
    "One way to do so is via **the cross validation set approach** which basically means setting aside a portion of\n",
    "our training data to use as a validation set. The goal is to use the validation set to find the best hyperparameters\n",
    "for our model (`max_depth` in the case of decision trees).\n",
    "\n",
    "- 3.1 [3 points] complete the `cross_validate` function to train the classifier on the training set and\n",
    "return the accuracy on the validation set based on provided indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "10a6a80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate(classifier, X, y, train_indices, valid_indices):\n",
    "    \"\"\"\n",
    "    Train classifier on training set and validate on the validation set\n",
    "    :param classifier: the classifier to use\n",
    "    :param X: all data of shape (num_samples, num_features)\n",
    "    :param y: all labels of shape (num_samples)\n",
    "    :param train_indices:  indices to be used for training the model\n",
    "    :param valid_indices:  indices to be used for validating the model\n",
    "    :return: he accuracy of the classifier on the validation set\n",
    "    \"\"\"\n",
    "    valid_accuracy = 0\n",
    "    #Workspace 3.1\n",
    "    #TODO: train and validate the model based on provided indices\n",
    "    #Hint: use score method of the classifier\n",
    "    #BEGIN \n",
    "\n",
    "    classifier.fit(np.array([X[index] for index in train_indices]), np.array([y[index] for index in train_indices]))\n",
    "    valid_accuracy = classifier.score(np.array([X[index] for index in valid_indices]), np.array([y[index] for index in valid_indices]))\n",
    "    \n",
    "    #END\n",
    "    return valid_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3be981d",
   "metadata": {},
   "source": [
    "- 3.2 [2 points] Report the validation accuracy using the validation set approach for scikit-learn `DecisionTreeClassifier` with `max_depth=3`\n",
    " when using the last 100 training points as a validation set and the rest as training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "89fc7e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.76\n"
     ]
    }
   ],
   "source": [
    "#Workspace 3.2\n",
    "#TODO: Report the cross validation accuracy using the last 100 training points as validation set\n",
    "#and the rest of the training points as training\n",
    "#BEGIN \n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "n_samples, n_features = bike_sharing.X_train.shape\n",
    "classifier = DecisionTreeClassifier(max_depth=3, min_samples_split=2)\n",
    "train_indices = [index for index in range(0, n_samples - 100)]\n",
    "valid_indices = [index for index in range(n_samples - 100, n_samples)]\n",
    "\n",
    "valid_accuracy = cross_validate(classifier, bike_sharing.X_train, bike_sharing.y_train, train_indices, valid_indices)\n",
    "\n",
    "print(valid_accuracy)\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c337cc29",
   "metadata": {},
   "source": [
    "The issue with the validation set approach is that we're reducing the size of our training data,\n",
    " and the lower number of samples implies higher uncertainty.\n",
    "\n",
    "A work-around is to use *k-fold cross validation*.\n",
    "We start by partitioning the training data into k different and equally size partitions.\n",
    "Then for each of the k runs, we keep a different chunk for the validation while using the remaining k-1 for training.\n",
    "We note the validation accuracy during each of the k runs.\n",
    "\n",
    "After each of the k-folds has been used as a validation set, the average of the k recorded accuracies becomes the performance of our model.\n",
    "The k-fold cross validation method gives us a better estimate on how well the model would perform on new unseen data\n",
    " (test set) while allowing it to train on a larger portion of the dataset.\n",
    "- 3.3 [5 points] Complete `k_fold_cv`. Use the helper function `generate_folds` that generates the partition of indices to k different chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bea027e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_folds(size, k):\n",
    "    \"\"\"\n",
    "    Shuffles and partition range(size) to to k contiguous chunks then generates the train/valid indices for the k-fold\n",
    "    To use as a generator, for an example run:\n",
    "        for train_idx, valid_idx in generate_folds(10,3): print(train_idx, valid_idx)\n",
    "    :param size: size of the range that should be split\n",
    "    :param k: number of folds\n",
    "    :return: iterable of different k splits, each is a tuple (train_indices, valid_indices)\n",
    "             where len(valid_indices)~ size/k\n",
    "    \"\"\"\n",
    "    permutation = np.random.RandomState(seed=42).permutation(size)\n",
    "    split_sizes = [size//k + (i < (size % k)) for i in range(k)] # we split the remainder amongst the first folds\n",
    "    start = 0\n",
    "    for i in range(k):\n",
    "        # valid indices of i-th split for which start <= σ < start + size_split[i]\n",
    "        # for_valid is True in position where condition is true, False otherwise\n",
    "        for_valid = np.logical_and(start<= permutation, permutation< start + split_sizes[i])\n",
    "        start += split_sizes[i] # update the start of the fold\n",
    "        valid_indices = np.where(for_valid)[0]\n",
    "        # train indices of i-th split for which σ <start or  start + size_split[i] <= σ\n",
    "        # ~bool_array is negation of bool_array\n",
    "        train_indices = np.where(~for_valid)[0]\n",
    "        yield train_indices, valid_indices\n",
    "\n",
    "def k_fold_cv(classifier, k, X, y):\n",
    "    \"\"\"\n",
    "    This function performs k-fold cross validation\n",
    "    :param classifier: a classifier to be used\n",
    "    :param k: number of folds\n",
    "    :param X: all training data of shape (num_samples, num_features)\n",
    "    :param y: all labels of shape (num_samples)\n",
    "    :return: the average accuracy of the classifier in k-runs\n",
    "    \"\"\"\n",
    "    mean_accuracy = 0\n",
    "    #Workspace 3.3\n",
    "    #BEGIN\n",
    "    n_samples, n_features = X.shape\n",
    "    \n",
    "    for train_indices, valid_indices in generate_folds(n_samples, k):\n",
    "        mean_accuracy += cross_validate(classifier, X, y, train_indices, valid_indices)\n",
    "    \n",
    "    mean_accuracy /= k\n",
    "    #END\n",
    "    return mean_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ef576d",
   "metadata": {},
   "source": [
    "- 3.4 [4 points] Consider depths from 1 to 10. Perform hyperparameter search by doing 8-fold cross validation for each depth. What is the best value of `max_depth` and what is the best cross validation accuracy you find over the validation splits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4df77689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current depth: 1, current accuracy: 0.5017123287671234\n",
      "current depth: 2, current accuracy: 0.6849315068493151\n",
      "current depth: 3, current accuracy: 0.7602739726027397\n",
      "current depth: 4, current accuracy: 0.7226027397260273\n",
      "current depth: 5, current accuracy: 0.7311643835616438\n",
      "current depth: 6, current accuracy: 0.75\n",
      "current depth: 7, current accuracy: 0.7568493150684932\n",
      "current depth: 8, current accuracy: 0.7534246575342466\n",
      "current depth: 9, current accuracy: 0.7517123287671234\n",
      "current depth: 10, current accuracy: 0.7226027397260274\n",
      "Cross validation accuracy for chosen best max_depth 3: 0.760274\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(4)  # changing the seed might yield different results\n",
    "best_depth, best_accuracy = -1, 0\n",
    "\n",
    "#Workspace 3.4\n",
    "#TODO: \n",
    "#BEGIN \n",
    "\n",
    "k = 8\n",
    "\n",
    "for depth in range(1, 11):\n",
    "    classifier = DecisionTreeClassifier(max_depth=depth, min_samples_split=2)\n",
    "    accuracy = k_fold_cv(classifier, k, bike_sharing.X_train, bike_sharing.y_train)\n",
    "    print(\"current depth: {}, current accuracy: {}\".format(depth, accuracy))\n",
    "    \n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_depth = depth\n",
    "    \n",
    "#END\n",
    "print(\"Cross validation accuracy for chosen best max_depth %d: %f\" % (best_depth, best_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b665a5d5",
   "metadata": {},
   "source": [
    "- 3.5 [2 pts] Train a new model on the entire training set with the best `max_depth` you found above. Report the accuracy of the new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b890ea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of the best model on the testing set 0.6802721088435374\n"
     ]
    }
   ],
   "source": [
    "test_accuracy = 0\n",
    "#Workspace 3.5\n",
    "#BEGIN \n",
    "test_accuracy = DecisionTreeClassifier(max_depth=best_depth, min_samples_split=2).\\\n",
    "    fit(bike_sharing.X_train, bike_sharing.y_train).\\\n",
    "    score(bike_sharing.X_test, bike_sharing.y_test)\n",
    "\n",
    "#END\n",
    "print (\"accuracy of the best model on the testing set\", test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656d47cb",
   "metadata": {},
   "source": [
    "Problem 4  - Decision Tree Ensembles: Bagging, Random Forests and Boosting [48 points]\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e84980",
   "metadata": {},
   "source": [
    "### Training Data for Decision tree Ensembles\n",
    "***\n",
    "Please do not change this class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5549a72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "from sklearn.base import clone\n",
    "from collections import Counter\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1dab322d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreesAndEights:\n",
    "    \"\"\"\n",
    "    Class to store MNIST data\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, location):\n",
    "\n",
    "        import pickle, gzip\n",
    "\n",
    "        # Load the dataset\n",
    "        f = gzip.open(location, 'rb')\n",
    "\n",
    "        # Split the data set \n",
    "#         X_train, y_train, X_valid, y_valid = pickle.load(f)\n",
    "        train_set, valid_set, test_set = pickle.load(f)\n",
    "    \n",
    "        X_train, y_train = train_set\n",
    "        X_valid, y_valid = valid_set\n",
    "\n",
    "        # Extract only 3's and 8's for training set \n",
    "        self.X_train = X_train[np.logical_or( y_train==3, y_train == 8), :]\n",
    "        self.y_train = y_train[np.logical_or( y_train==3, y_train == 8)]\n",
    "        self.y_train = np.array([1 if y == 8 else -1 for y in self.y_train])\n",
    "        \n",
    "        # Shuffle the training data \n",
    "        shuff = np.arange(self.X_train.shape[0])\n",
    "        np.random.shuffle(shuff)\n",
    "        self.X_train = self.X_train[shuff,:]\n",
    "        self.y_train = self.y_train[shuff]\n",
    "\n",
    "        # Extract only 3's and 8's for validation set \n",
    "        self.X_valid = X_valid[np.logical_or( y_valid==3, y_valid == 8), :]\n",
    "        self.y_valid = y_valid[np.logical_or( y_valid==3, y_valid == 8)]\n",
    "        self.y_valid = np.array([1 if y == 8 else -1 for y in self.y_valid])\n",
    "        \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "91f5bc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ThreesAndEights(\"data/mnist.pklz\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598a7edc",
   "metadata": {},
   "source": [
    "Feel free to explore this data and get comfortable with it before proceeding further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccafcd11",
   "metadata": {},
   "source": [
    "### Bagging\n",
    "Bootstrap aggregating, also called bagging, is a machine learning ensemble meta-algorithm designed to improve the stability and accuracy of machine learning algorithms used in statistical classification and regression. It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method. Bagging is a special case of the model averaging approach.\n",
    "\n",
    "Given a standard training set $D$ of size n, bagging generates $N$ new training sets $D_i$, roughly each of size n * ratio, by sampling from $D$ uniformly and with replacement. By sampling with replacement, some observations may be repeated in each $D_i$ The $N$ models are fitted using the above $N$ bootstraped samples and combined by averaging the output (for regression) or voting (for classification). \n",
    "\n",
    "-Source [Wiki](https://en.wikipedia.org/wiki/Bootstrap_aggregating)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a662c0e",
   "metadata": {},
   "source": [
    "### Problems 4.1-4.4: Implementing Bagging [19-points]\n",
    "***\n",
    "\n",
    "We've given you a skeleton of the class `BaggingClassifier` below which will train a classifier based on the decision trees as implemented by sklearn. Your tasks are as follows, please approach step by step to understand the code flow:\n",
    "* 4.1 [5 points] Implement `bootstrap` method which takes in two parameters (`X_train, y_train`) and returns a bootstrapped training set ($D_i$)\n",
    "* 4.2 [5 points] Implement `fit` method which takes in two parameters (`X_train, y_train`) and trains `N` number of base models on different bootstrap samples. You should call `bootstrap` method to get bootstrapped training data for each of your base model\n",
    "* 4.3 [4 points] Implement `voting` method which takes the predictions from learner trained on bootstrapped data points `y_hats` and returns final prediction as per majority rule. In case of ties, return either of the class randomly.\n",
    "* 4.4 [5 points] Implement `predict` method which takes in multiple data points and returns final prediction for each one of those. Please use the `voting` method to reach consensus on final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8be4c48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import random\n",
    "\n",
    "class BaggingClassifier:\n",
    "    def __init__(self, ratio = 0.73, N = 20, base=DecisionTreeClassifier(max_depth=4)):\n",
    "        \"\"\"\n",
    "        Create a new BaggingClassifier\n",
    "        \n",
    "        Args:\n",
    "            base (BaseEstimator, optional): Sklearn implementation of decision tree\n",
    "            ratio: ratio of number of data points in subsampled data to the actual training data\n",
    "            N: number of base estimator in the ensemble\n",
    "        \n",
    "        Attributes:\n",
    "            base (estimator): Sklearn implementation of decision tree\n",
    "            N: Number of decision trees\n",
    "            learners: List of models trained on bootstrapped data sample\n",
    "        \"\"\"\n",
    "        \n",
    "        assert ratio <= 1.0, \"Cannot have ratio greater than one\"\n",
    "        self.base = base\n",
    "        self.ratio = ratio\n",
    "        self.N = N\n",
    "        self.learners = []\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Train Bagging Ensemble Classifier on data\n",
    "        \n",
    "        Args:\n",
    "            X_train (ndarray): [n_samples x n_features] ndarray of training data   \n",
    "            y_train (ndarray): [n_samples] ndarray of data \n",
    "        \"\"\"\n",
    "        #TODO: Implement functionality to fit models on the bootstrapped samples\n",
    "        # cloning sklearn models:\n",
    "        # from sklearn.base import clone        \n",
    "        #BEGIN        \n",
    "        for i in range(self.N):\n",
    "            h = clone(self.base)\n",
    "            X_sample, y_sample = self.bootstrap(X_train, y_train)\n",
    "            learner = h.fit(X_sample, y_sample)\n",
    "            self.learners.append(learner)\n",
    "        #END\n",
    "        \n",
    "    def bootstrap(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n (int): total size of the training data\n",
    "            X_train (ndarray): [n_samples x n_features] ndarray of training data   \n",
    "            y_train (ndarray): [n_samples] ndarray of data\n",
    "        \n",
    "        \n",
    "        \n",
    "        Returns:\n",
    "            X_train (ndarray): [ratio*n_samples x n_features] ndarray of training data\n",
    "            y_train (ndarray): [ratio*n_samples] ndarray of training labels\n",
    "        \"\"\"\n",
    "        #TODO: Implement functionality to sample (with replacement) n * ratio\n",
    "        # number of data points from input examples\n",
    "        #BEGIN\n",
    "        n_samples, n_features = X_train.shape\n",
    "        sample_indicies = np.random.choice(n_samples, int(n_samples * self.ratio))\n",
    "        \n",
    "        X_sample = np.array([X_train[index] for index in sample_indicies])\n",
    "        y_sample = np.array([y_train[index] for index in sample_indicies])\n",
    "        return X_sample, y_sample\n",
    "        #END\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        BaggingClassifier prediction for data points in X\n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): [n_samples x n_features] ndarray of data \n",
    "            \n",
    "        Returns:\n",
    "            yhat (ndarray): [n_samples] ndarray of predicted labels {-1,1}\n",
    "        \"\"\"\n",
    "        \n",
    "        #TODO: Using the individual classifiers trained predict the final prediction using voting mechanism\n",
    "        #BEGIN\n",
    "        n_samples, n_features = X.shape\n",
    "        y_hats = np.zeros((n_samples, self.N), dtype=int)\n",
    "        y_hat = np.zeros((n_samples), dtype=int)\n",
    "        \n",
    "        for i in range(self.N):\n",
    "            y_hats[:, i] = self.learners[i].predict(X)\n",
    "         \n",
    "        for i in range(n_samples):\n",
    "            y_hat[i] = self.voting(y_hats[i])\n",
    "        \n",
    "        return y_hat\n",
    "        #END\n",
    "        \n",
    "    def voting(self, y_hats):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y_hats (ndarray): [N] ndarray of data\n",
    "        Returns:\n",
    "            y_final : int, final prediction of the sample\n",
    "        \"\"\"\n",
    "        #TODO: Implement majority voting scheme and incase of ties return random label\n",
    "        #BEGIN\n",
    "        unique_labels, label_counts = np.unique(y_hats, return_counts=True)\n",
    "        \n",
    "        max_label_counts = max(label_counts) \n",
    "        indicies = [i for i in range(len(label_counts)) if label_counts[i] == max_label_counts]\n",
    "        \n",
    "        winners = unique_labels[indicies]\n",
    "        # if there is one winner, random choice will return that value only.\n",
    "        # otherwise, it will randomly pick a winner incase of ties\n",
    "        y_final = random.choice(winners)\n",
    "\n",
    "        return y_final\n",
    "        #END\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Return the mean accuracy on the given test data and labels.\n",
    "        :param X: Test samples, shape (num_points, num_features)\n",
    "        :param y: true labels for X, shape (num_points,)\n",
    "        :return: mean accuracy\n",
    "        \"\"\"\n",
    "        accuracy = 0\n",
    "        # Workspace 1.7\n",
    "        #BEGIN\n",
    "        y_hat = self.predict(X)\n",
    "        for i in range(len(y)):\n",
    "            if y_hat[i] == y[i]:\n",
    "                accuracy += 1\n",
    "        \n",
    "        accuracy /= len(y)\n",
    "        #END\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba18446",
   "metadata": {},
   "source": [
    "### Problem 4.5 - BaggingClassifier for Handwritten Digit Recognition [5 points]\n",
    "***\n",
    "\n",
    "4.5 a [2 points] After you've successfully completed `BaggingClassifier` find the optimal values of `N` and `depth` using k-fold cross validation. You are allowed to use sklearn library to split your training data in folds. Keep the other hyperparameters unchanged. Use the data from `ThreesAndEights` class initialized variable `data`. \n",
    "\n",
    "Hint:  Vary `depth` up to 10, `N` up to 40. The number of decision trees `N` is generally a trade-off between 'improvement in accuracy' vs 'computation time'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "eb48c262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current depth: 3, current N: 10, current accuracy: 0.939455637560678\n",
      "current depth: 3, current N: 13, current accuracy: 0.9450871741656659\n",
      "current depth: 3, current N: 16, current accuracy: 0.9396566019305534\n",
      "current depth: 3, current N: 20, current accuracy: 0.9452882195042642\n",
      "current depth: 3, current N: 23, current accuracy: 0.9426735775090912\n",
      "current depth: 3, current N: 26, current accuracy: 0.9399582913915351\n",
      "current depth: 3, current N: 30, current accuracy: 0.9409644087404765\n",
      "current depth: 3, current N: 33, current accuracy: 0.9407622298397597\n",
      "current depth: 3, current N: 36, current accuracy: 0.9431764742461164\n",
      "current depth: 3, current N: 40, current accuracy: 0.9396570067741673\n",
      "current depth: 4, current N: 10, current accuracy: 0.9566533133049101\n",
      "current depth: 4, current N: 13, current accuracy: 0.9607764026049906\n",
      "current depth: 4, current N: 16, current accuracy: 0.9599724641567658\n",
      "current depth: 4, current N: 20, current accuracy: 0.960876965758651\n",
      "current depth: 4, current N: 23, current accuracy: 0.9588656217167183\n",
      "current depth: 4, current N: 26, current accuracy: 0.9599720593131521\n",
      "current depth: 4, current N: 30, current accuracy: 0.9605755192038378\n",
      "current depth: 4, current N: 33, current accuracy: 0.9594692435448496\n",
      "current depth: 4, current N: 36, current accuracy: 0.9573572553805336\n",
      "current depth: 4, current N: 40, current accuracy: 0.9601733475579186\n",
      "current depth: 5, current N: 10, current accuracy: 0.9667105193269101\n",
      "current depth: 5, current N: 13, current accuracy: 0.9687220253062886\n",
      "current depth: 5, current N: 16, current accuracy: 0.9701298284888127\n",
      "current depth: 5, current N: 20, current accuracy: 0.971135702931586\n",
      "current depth: 5, current N: 23, current accuracy: 0.9692245981684227\n",
      "current depth: 5, current N: 26, current accuracy: 0.9682190476005405\n",
      "current depth: 5, current N: 30, current accuracy: 0.9685204941553537\n",
      "current depth: 5, current N: 33, current accuracy: 0.9699287831502145\n",
      "current depth: 5, current N: 36, current accuracy: 0.9707335312856666\n",
      "current depth: 5, current N: 40, current accuracy: 0.9702304726111959\n",
      "current depth: 6, current N: 10, current accuracy: 0.9754604853200467\n",
      "current depth: 6, current N: 13, current accuracy: 0.9762649905493307\n",
      "current depth: 6, current N: 16, current accuracy: 0.976264909580608\n",
      "current depth: 6, current N: 20, current accuracy: 0.9768683694712936\n",
      "current depth: 6, current N: 23, current accuracy: 0.9761643464269475\n",
      "current depth: 6, current N: 26, current accuracy: 0.9765665990415896\n",
      "current depth: 6, current N: 30, current accuracy: 0.9760637023045642\n",
      "current depth: 6, current N: 33, current accuracy: 0.9769689326249542\n",
      "current depth: 6, current N: 36, current accuracy: 0.9768682885025709\n",
      "current depth: 6, current N: 40, current accuracy: 0.9768682885025709\n",
      "current depth: 7, current N: 10, current accuracy: 0.9792821280653138\n",
      "current depth: 7, current N: 13, current accuracy: 0.984913988545193\n",
      "current depth: 7, current N: 16, current accuracy: 0.9820980583052534\n",
      "current depth: 7, current N: 20, current accuracy: 0.979583898495018\n",
      "current depth: 7, current N: 23, current accuracy: 0.9816956437531659\n",
      "current depth: 7, current N: 26, current accuracy: 0.9797847818961709\n",
      "current depth: 7, current N: 30, current accuracy: 0.9813940352609071\n",
      "current depth: 7, current N: 33, current accuracy: 0.9838076319174818\n",
      "current depth: 7, current N: 36, current accuracy: 0.982500472857341\n",
      "current depth: 7, current N: 40, current accuracy: 0.979483173403912\n",
      "current depth: 8, current N: 10, current accuracy: 0.9846125419903796\n",
      "current depth: 8, current N: 13, current accuracy: 0.9831039327480267\n",
      "current depth: 8, current N: 16, current accuracy: 0.987126944706783\n",
      "current depth: 8, current N: 20, current accuracy: 0.9853164840660029\n",
      "current depth: 8, current N: 23, current accuracy: 0.9864227597249915\n",
      "current depth: 8, current N: 26, current accuracy: 0.986322115602608\n",
      "current depth: 8, current N: 30, current accuracy: 0.9869257374307394\n",
      "current depth: 8, current N: 33, current accuracy: 0.9867245301546955\n",
      "current depth: 8, current N: 36, current accuracy: 0.9866238860323122\n",
      "current depth: 8, current N: 40, current accuracy: 0.9856179306208163\n",
      "current depth: 9, current N: 10, current accuracy: 0.987529197321425\n",
      "current depth: 9, current N: 13, current accuracy: 0.9871269447067832\n",
      "current depth: 9, current N: 16, current accuracy: 0.9869255754932937\n",
      "current depth: 9, current N: 20, current accuracy: 0.9878310487198522\n",
      "current depth: 9, current N: 23, current accuracy: 0.9871267827693376\n",
      "current depth: 9, current N: 26, current accuracy: 0.9890375636576099\n",
      "current depth: 9, current N: 30, current accuracy: 0.9883335406132635\n",
      "current depth: 9, current N: 33, current accuracy: 0.9870260576782316\n",
      "current depth: 9, current N: 36, current accuracy: 0.9884343466730923\n",
      "current depth: 9, current N: 40, current accuracy: 0.9883336215819862\n",
      "current depth: 10, current N: 10, current accuracy: 0.9876299224125312\n",
      "current depth: 10, current N: 13, current accuracy: 0.9872271839855525\n",
      "current depth: 10, current N: 16, current accuracy: 0.9892387709336535\n",
      "current depth: 10, current N: 20, current accuracy: 0.9896411854857411\n",
      "current depth: 10, current N: 23, current accuracy: 0.9895407033008032\n",
      "current depth: 10, current N: 26, current accuracy: 0.9894399782096974\n",
      "current depth: 10, current N: 30, current accuracy: 0.9900435190691057\n",
      "current depth: 10, current N: 33, current accuracy: 0.9897417486394017\n",
      "current depth: 10, current N: 36, current accuracy: 0.9895407033008035\n",
      "current depth: 10, current N: 40, current accuracy: 0.9901440012540436\n",
      "Cross validation accuracy for chosen best_depth 10 best_n 40: 0.990144\n"
     ]
    }
   ],
   "source": [
    "# Just an example, change accordingly\n",
    "ratios = np.logspace(1, 5, 3, base = .9)\n",
    "depths = np.linspace(3, 10, 8, dtype = int)\n",
    "Ns = np.linspace(10, 40, 10, dtype = int)\n",
    "\n",
    "#Workspace 4.5\n",
    "#BEGIN\n",
    "# np.random.seed(4)  # changing the seed might yield different results\n",
    "best_depth = -1\n",
    "best_n = -1\n",
    "best_accuracy = 0\n",
    "\n",
    "k = 8\n",
    "\n",
    "for depth in depths:\n",
    "    for N in Ns:\n",
    "        classifier = BaggingClassifier(ratio = 0.73, N = N, base=DecisionTreeClassifier(max_depth=depth))\n",
    "        accuracy = k_fold_cv(classifier, k, data.X_train, data.y_train)\n",
    "        print(\"current depth: {}, current N: {}, current accuracy: {}\".format(depth, N, accuracy))\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_depth = depth\n",
    "            best_n = N\n",
    "\n",
    "print(\"Cross validation accuracy for chosen best_depth %d best_n %d: %f\" % (best_depth, best_n, best_accuracy))\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c005af",
   "metadata": {},
   "source": [
    "4.5 b [2 points] After finding optimal parameters using k-fold cross-validation,report accuracy on the validation data using the optimal parameter values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bb6a27ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on the validation data using the optimal parameter values is 0.9754781755762629\n"
     ]
    }
   ],
   "source": [
    "#Workspace 4.5 b\n",
    "#BEGIN\n",
    "best_n = 40\n",
    "best_depth = 10\n",
    "classifier = BaggingClassifier(ratio = 0.73, N = best_n, base=DecisionTreeClassifier(max_depth=best_depth))\n",
    "classifier.fit(data.X_train, data.y_train)                                           \n",
    "accuracy = classifier.score(data.X_valid, data.y_valid)\n",
    "print(\"Accuracy on the validation data using the optimal parameter values is {}\".format(str(accuracy)))\n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587a6b4d",
   "metadata": {},
   "source": [
    "4.5 c [1 point] What is the most deciding hyperparameter? Give us a brief intuition as to why that might be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e757e204",
   "metadata": {},
   "source": [
    "% Write-up for 4.5.c <br>\n",
    "#BEGIN <br>\n",
    "The most deciding hyperparameter is the number of base estimator in the ensemble. Increasing the number of base estimators can help to improve the BaggingClassifier's robustness by reducing the impact of any individual tree's biases and overfitting problems.\n",
    "\n",
    "#END<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c11df9",
   "metadata": {},
   "source": [
    "### Problems 4.6-4.8 Implementing Random Forest [12 points]\n",
    "Random Forest has an additional layer of randomness compared to Bagging: we also sample a random subset of the features (columns). The rest of the implementation should be similar if not exactly the same as Bagging. In addition to keeping track of the estimators (in RandomForest.estimators, we also have to store the features indices that are used by each estimator (in RandomForest.features_indices).\n",
    "\n",
    "4.6 [4 points] First, complete `bootstrap` to return a random sample of size sample_ratio* len(X_train) of labels and feature_ratio * num_features of features\n",
    "\n",
    "4.7 [4 points] Complete `fit` by building n_estimators of DecisionTreeClassifier, each trained on random sample of the data. Make sure to keep track of the sampled features for each estimator to use them in the prediction step\n",
    "\n",
    "4.8 [4 points] Complete `predict` method to return the most likely label by combining different estimators predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9c37628",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest(BaggingClassifier):\n",
    "    def __init__(self, N = 20, samples_ratio = 0.73, features_ratio = 1.0,base=DecisionTreeClassifier(max_depth=4)):\n",
    "        \"\"\"\n",
    "        Create a new BaggingClassifier\n",
    "        \n",
    "        Args:\n",
    "            N: number of base estimator in the ensemble\n",
    "            samples_ratio: ratio of number of data points in subsampled data to the actual training data\n",
    "            features_ratio: ratio of number of features to be considered in each sample\n",
    "            base (BaseEstimator, optional): Sklearn implementation of decision tree\n",
    "        \n",
    "        Attributes:\n",
    "            base (estimator): Sklearn implementation of decision tree\n",
    "            N: Number of decision trees\n",
    "            learners: List of models trained on bootstrapped data sample\n",
    "            samples_ratio: ratio of number of data points in subsampled data to the actual training data\n",
    "            features_ratio: ratio of number of features to be considered in each sample\n",
    "            features_indices_array = array to keep track of feature indices sampled for each learner model\n",
    "        \"\"\"\n",
    "        \n",
    "        assert samples_ratio <= 1.0, \"Cannot have ratio greater than one\"\n",
    "        assert features_ratio <= 1.0, \"Cannot have ratio greater than one\"\n",
    "        self.base = base\n",
    "        self.samples_ratio = samples_ratio\n",
    "        self.N = N\n",
    "        self.learners = []\n",
    "        self.features_ratio = features_ratio\n",
    "        self.features_indices_array = []\n",
    "    \n",
    "    def bootstrap(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n (int): total size of the training data\n",
    "            X_train (ndarray): [n_samples x n_features] ndarray of training data   \n",
    "            y_train (ndarray): [n_samples] ndarray of data \n",
    "        Returns:\n",
    "            X_train (ndarray): [samples_ratio*n_samples x features_ratio*n_features] ndarray of training data   \n",
    "            y_train (ndarray): [samples_ratio*n_samples] ndarray of data\n",
    "            features_indices: indices of the features sampled to be used in other functions\n",
    "        \"\"\"\n",
    "        #TODO: Implement functionality to sample (with replacement) n_samples* samples_ratio number of data points \n",
    "        # from input examples and inturn sample n_features* features_ratio number of features from each data point\n",
    "        #Workspace 4.6\n",
    "        #BEGIN\n",
    "        n_samples, n_features = X_train.shape\n",
    "        sample_indicies = np.random.choice(n_samples, int(n_samples * self.samples_ratio))\n",
    "        feature_indices = np.random.choice(n_features, int(n_features * self.features_ratio))\n",
    "        \n",
    "        X_sample = np.array([X_train[index] for index in sample_indicies])\n",
    "        y_sample = np.array([y_train[index] for index in sample_indicies])\n",
    "        return X_sample, y_sample, feature_indices\n",
    "        \n",
    "        #END\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        \"\"\"\n",
    "        Train Bagging Ensemble Classifier on data\n",
    "        \n",
    "        Args:\n",
    "            X_train (ndarray): [n_samples x n_features] ndarray of training data   \n",
    "            y_train (ndarray): [n_samples] ndarray of data \n",
    "        \"\"\"\n",
    "        #TODO: Implement functionality to fit models on the bootstrapped samples\n",
    "        # cloning sklearn models:\n",
    "        # from sklearn.base import clone\n",
    "        # h = clone(self.base)\n",
    "        #Workspace 4.7\n",
    "        #BEGIN        \n",
    "        \n",
    "        for i in range(self.N):\n",
    "            h = clone(self.base)\n",
    "            \n",
    "            X_sample, y_sample, feature_indices = self.bootstrap(X_train, y_train)\n",
    "            self.features_indices_array.append(feature_indices)\n",
    "            \n",
    "            learner = h.fit(X_sample[:, feature_indices], y_sample)\n",
    "            self.learners.append(learner)\n",
    "        #END\n",
    "\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        BaggingClassifier prediction for data points in X\n",
    "        \n",
    "        Args:\n",
    "            X (ndarray): [n_samples x n_features] ndarray of data \n",
    "            \n",
    "        Returns:\n",
    "            yhat (ndarray): [n_samples] ndarray of predicted labels {-1,1}\n",
    "        \"\"\"\n",
    "        #TODO: compute cumulative sum of predict proba from estimators and return the labels with highest likelihood\n",
    "        #Workspace 4.8\n",
    "        #BEGIN\n",
    "        n_samples, n_features = X.shape\n",
    "        y_hats = np.zeros((n_samples, self.N), dtype=int)\n",
    "        y_hat = np.zeros((n_samples), dtype=int)\n",
    "        \n",
    "        for i in range(self.N):\n",
    "            y_hats[:, i] = self.learners[i].predict(X[:, self.features_indices_array[i]])\n",
    "         \n",
    "        for i in range(n_samples):\n",
    "            y_hat[i] = self.voting(y_hats[i])\n",
    "        \n",
    "        return y_hat\n",
    "        #END\n",
    "        \n",
    "    def voting(self, y_hats):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y_hats (ndarray): [N] ndarray of data\n",
    "        Returns:\n",
    "            y_final : int, final prediction of the sample\n",
    "        \"\"\"\n",
    "        #TODO: Implement majority voting scheme and incase of ties return random label\n",
    "        #BEGIN\n",
    "        unique_labels, label_counts = np.unique(y_hats, return_counts=True)\n",
    "        \n",
    "        max_label_counts = max(label_counts) \n",
    "        indicies = [i for i in range(len(label_counts)) if label_counts[i] == max_label_counts]\n",
    "        \n",
    "        winners = unique_labels[indicies]\n",
    "        # if there is one winner, random choice will return that value only.\n",
    "        # otherwise, it will randomly pick a winner incase of ties\n",
    "        y_final = random.choice(winners)\n",
    "\n",
    "        return y_final\n",
    "        #END\n",
    "        \n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Return the mean accuracy on the given test data and labels.\n",
    "        :param X: Test samples, shape (num_points, num_features)\n",
    "        :param y: true labels for X, shape (num_points,)\n",
    "        :return: mean accuracy\n",
    "        \"\"\"\n",
    "        accuracy = 0\n",
    "        # Workspace 1.7\n",
    "        #BEGIN\n",
    "        y_hat = self.predict(X)\n",
    "        for i in range(len(y)):\n",
    "            if y_hat[i] == y[i]:\n",
    "                accuracy += 1\n",
    "        \n",
    "        accuracy /= len(y)\n",
    "        #END\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca44a47",
   "metadata": {},
   "source": [
    "### Problems 4.9 Experimenting with Random Forest [2 points]\n",
    "4.9 [2 points] Keep the optimal hyperparameters for `N` and `depth` found in `BaggingClassifier`, please run K-fold cross-validation with a range of `features_ratio` and report an approximate ratio at which the accuracy stops to improve significantly. You may use any kind of tools necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "662749cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current feature ratio: 0.5, current accuracy: 0.991954299957378\n",
      "current feature ratio: 0.3674336230688997, current accuracy: 0.992155102389808\n",
      "current feature ratio: 0.2700149347230765, current accuracy: 0.9943680585513983\n",
      "current feature ratio: 0.19842513149602498, current accuracy: 0.9941668512753545\n",
      "current feature ratio: 0.1458161299470146, current accuracy: 0.9943680585513983\n",
      "current feature ratio: 0.1071554978566341, current accuracy: 0.9938649998769276\n",
      "current feature ratio: 0.07874506561842957, current accuracy: 0.9923568764469111\n",
      "current feature ratio: 0.05786716951795567, current accuracy: 0.9933624270147934\n",
      "current feature ratio: 0.042524687505449285, current accuracy: 0.9893398198996506\n",
      "current feature ratio: 0.03125, current accuracy: 0.9860208309852403\n",
      "Cross validation accuracy for chosen best_feature_ratio 0.270015: 0.994368\n"
     ]
    }
   ],
   "source": [
    "# Just an example, change accordingly\n",
    "features_ratios = np.logspace(1, 5, 10, base = .5)\n",
    "\n",
    "#Workspace 4.9\n",
    "#BEGIN\n",
    "best_feature_ratio = -1\n",
    "best_accuracy = 0\n",
    "\n",
    "best_depth = 10 \n",
    "best_n = 40\n",
    "k = 8\n",
    "\n",
    "for features_ratio in features_ratios:\n",
    "    classifier = RandomForest(N = best_n, samples_ratio = 0.73, features_ratio = features_ratio, base=DecisionTreeClassifier(max_depth=best_depth))\n",
    "    accuracy = k_fold_cv(classifier, k, data.X_train, data.y_train)\n",
    "    print(\"current feature ratio: {}, current accuracy: {}\".format(features_ratio, accuracy))\n",
    "\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_feature_ratio = features_ratio        \n",
    "\n",
    "print(\"Cross validation accuracy for chosen best_feature_ratio %f: %f\" % (best_feature_ratio, best_accuracy))    \n",
    "#END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17408282",
   "metadata": {},
   "source": [
    "#BEGIN <br>\n",
    "Since I am using depth=10 and N=40 for the experiment, it's hard to see significant increase on the performance. However, you can see that the accuracy stops to improve when feature ratio is 0.270015\n",
    "\n",
    "#END<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404ba8b1",
   "metadata": {},
   "source": [
    "**Boosting**\n",
    "\n",
    "There are different methods of boosting, but we'll focus in this problem on Adaptive Boosting (AdaBoost).\n",
    "The logic of AdaBoost is to \"push\" each new learner to give more importance to previously misclassified data. We present\n",
    "below the multiclass variant of AdaBoost [SAMME](https://web.stanford.edu/~hastie/Papers/samme.pdf). We denote $K$ the number of classes.\n",
    "\n",
    "AdaBosst is performed by increasing the weights of misclassified simple after each iteration:\n",
    "- Input: m samples $(X_i, y_i)_{i\\in [m]}$, number of boosting rounds $N$\n",
    "- Start with equal samples weights $W = (w_i), $ where   $w_i = \\frac{1}{\\texttt{n_samples}}$\n",
    "- at round j:\n",
    "    - Train estimator $h_j$ using current weights $W$\n",
    "    - Get the predicted $(\\hat{y}_i)$ on the training data using $h_j$\n",
    "    - Find the weighted error rate $\\epsilon_j$ using $W$: $\\epsilon_j=\\frac{\\sum_i w_i \\Delta(\\hat{y}_i, y_i)}{\\sum_i w_i}$\n",
    "    - Choose $\\alpha_j = \\log \\frac{1-\\epsilon_j}{\\epsilon_j} + \\log(K-1)$\n",
    "    - Update $W$ using: $w_i \\leftarrow w_i \\exp(\\alpha_j \\Delta(\\hat{y_i}, y_i)) $\n",
    "    - Normalize $W$ to have sum 1\n",
    "- Global estimator is $H = \\sum_j \\alpha_j h_j$,\n",
    "\n",
    "the $\\Delta$ function equals to 1 when the two argument are different, 0 otherwise.\n",
    "\n",
    "To understand how we implement $H$, imagine we have two classes, and we boosted for 3 rounds to get $(h_1, h_2, h_3)$,\n",
    "with weights $(\\alpha_1, \\alpha_2, \\alpha_3)$. When we want to predict the label of sample $x$, we get $(h_1(x), h_2(x), h_3(x)) = (0,1,0)$.\n",
    "\n",
    "In this case, label $0$ gets a weight $\\alpha_1+\\alpha_3$, while class $1$ get weight $\\alpha_2$. The predicted class is the one with\n",
    "the largest weight (1 if $\\alpha_2 > \\alpha_1 + \\alpha_3$, 0 otherwise)\n",
    "\n",
    "### Problems 4.10- 4.11 Implementing Boosting - 6 points\n",
    "\n",
    "- 4.10 [4 pts] Complete `fit` by building `n_estimators` of DecisionTreeClassifier, each trained on the same data but with different samples weights as detailed in the algorithm. Keep track of $(\\alpha_i)$\n",
    "\n",
    "_Hint: our weak learner (DecisionTreeClassifier) can take an argument `sample_weight` when calling the `fit` method, you'll have to use it to provide the weights $W$_\n",
    "\n",
    "- 4.11  [2 pts] Complete `predict` method to return the predicted label using the global estimator $H$. \n",
    "\n",
    "_Hint: use one hot encoding of the predicted labels from the weak learners and cumulate the prediction with weights $\\alpha_j$, a dictionary will also work_\n",
    "\n",
    "Notice that if the estimator is consistent (0 error rate on the training set), AdaBoost $\\alpha_j$ are no longer defined. That's why this method requires a **weak** learner like the base model we used in the previous problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6f78a71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaBoost(object):\n",
    "\n",
    "    def __init__(self, n_estimators, base = DecisionTreeClassifier(max_depth=4)):\n",
    "        \"\"\"\n",
    "        :param n_estimators: number of estimators/ boosting rounds\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.num_classes = None\n",
    "        self.estimators = []\n",
    "        self.alphas = np.zeros(n_estimators)\n",
    "        self.base = base\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "\n",
    "        self.num_classes = np.unique(y_train).shape[0] # K in the algorithm\n",
    "        weights = np.ones(len(X_train)) / len(X_train) # W in the algorithm\n",
    "        # Workspace 4.10\n",
    "        #TODO: Implement Multiclass Adaboost and keep track of the alpha_j\n",
    "        #BEGIN\n",
    "        self.estimators = []\n",
    "        for j in range(self.n_estimators):\n",
    "            h = clone(self.base)\n",
    "            h.fit(X_train, y_train, sample_weight=weights)\n",
    "            self.estimators.append(h)\n",
    "            \n",
    "            y_pred = h.predict(X_train)\n",
    "            error = np.sum(weights * (y_pred != y_train)) / np.sum(weights)\n",
    "            \n",
    "            alpha = np.log((1 - error) / error) + np.log(self.num_classes - 1)\n",
    "            self.alphas[j] = alpha\n",
    "            \n",
    "            weights = weights * np.exp(alpha * (y_pred != y_train))\n",
    "            # Normalize W to have sum 1\n",
    "            weights = weights / np.sum(weights)\n",
    "        #END\n",
    "\n",
    "    def predict(self, X_test):\n",
    "        answer = 0\n",
    "        # Workspace 4.11\n",
    "        #TODO: get the labels returned by the global estimator defined as H\n",
    "        #Hint: Use one-hot format to accumulate alphas for different classes, or a dictionary\n",
    "        # The predicted label is the one that accumulates the largest sum of alphas\n",
    "        #Hint: We don't need predict_proba for this one\n",
    "        #BEGIN\n",
    "        # y_hat is one-hot encoding of the multi-class labels\n",
    "        # np.eye(k)[i] returns the one-hot encoding of size k for label i\n",
    "        # np.eye(4)[2] would be [0, 0, 1, 0], np.eye(4)[0] is [1,0, 0, 0]. Clever, innit?\n",
    "        \n",
    "        answers = np.zeros((len(X_test), self.num_classes))\n",
    "        for j, estimator in enumerate(self.estimators):\n",
    "            y_hat = estimator.predict(X_test)\n",
    "            y_hat_onehot = np.zeros((len(y_hat), self.num_classes), dtype=int)\n",
    "            y_hat_onehot[np.arange(len(y_hat)), y_hat] = 1\n",
    "\n",
    "            answers += self.alphas[j] * y_hat_onehot\n",
    "\n",
    "        answer = np.argmax(answers, axis=1)\n",
    "        return answer\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"\n",
    "        Return the mean accuracy on the given test data and labels.\n",
    "        :param X: Test samples, shape (num_points, num_features)\n",
    "        :param y: true labels for X, shape (num_points,)\n",
    "        :return: mean accuracy\n",
    "        \"\"\"\n",
    "        accuracy = 0\n",
    "        # Workspace 1.7\n",
    "        #BEGIN\n",
    "        y_hat = self.predict(X)\n",
    "        for i in range(len(y)):\n",
    "            if y_hat[i] == y[i]:\n",
    "                accuracy += 1\n",
    "        \n",
    "        accuracy /= len(y)\n",
    "        #END\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f670afea",
   "metadata": {},
   "source": [
    "### Problem 4.12 Hyperparameter tuning on Adaboost [4 points]\n",
    "4.12 a [2 points] Try out different values for `max_depth` in base model and `n_estimators` to arrive at the best evaluation metrics on `ThreesAndEights` validation set after training on `ThreesAndEights` training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "36e07c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current depth: 3, current n_estimator: 10, current accuracy: 0.9685213848113041\n",
      "current depth: 3, current n_estimator: 13, current accuracy: 0.973047212538363\n",
      "current depth: 3, current n_estimator: 16, current accuracy: 0.974354695473395\n",
      "current depth: 3, current n_estimator: 20, current accuracy: 0.977070062559674\n",
      "current depth: 3, current n_estimator: 23, current accuracy: 0.9797855106146756\n",
      "current depth: 3, current n_estimator: 26, current accuracy: 0.9810928316122621\n",
      "current depth: 3, current n_estimator: 30, current accuracy: 0.9827020040082756\n",
      "current depth: 3, current n_estimator: 33, current accuracy: 0.9825007967322319\n",
      "current depth: 3, current n_estimator: 36, current accuracy: 0.9829030493468738\n",
      "current depth: 3, current n_estimator: 40, current accuracy: 0.9833053829302386\n",
      "current depth: 4, current n_estimator: 10, current accuracy: 0.9764660358879289\n",
      "current depth: 4, current n_estimator: 13, current accuracy: 0.9792825329089275\n",
      "current depth: 4, current n_estimator: 16, current accuracy: 0.9801878441980405\n",
      "current depth: 4, current n_estimator: 20, current accuracy: 0.9819983048388203\n",
      "current depth: 4, current n_estimator: 23, current accuracy: 0.9834059460838991\n",
      "current depth: 4, current n_estimator: 26, current accuracy: 0.9837076355448806\n",
      "current depth: 4, current n_estimator: 30, current accuracy: 0.9854176949694455\n",
      "current depth: 4, current n_estimator: 33, current accuracy: 0.9857191415242588\n",
      "current depth: 4, current n_estimator: 36, current accuracy: 0.9867246111234185\n",
      "current depth: 4, current n_estimator: 40, current accuracy: 0.9870263815531226\n",
      "current depth: 6, current n_estimator: 10, current accuracy: 0.9828021623183224\n",
      "current depth: 6, current n_estimator: 13, current accuracy: 0.9841094023471861\n",
      "current depth: 6, current n_estimator: 16, current accuracy: 0.9870263815531226\n",
      "current depth: 6, current n_estimator: 20, current accuracy: 0.9872274268917209\n",
      "current depth: 6, current n_estimator: 23, current accuracy: 0.9899428749467225\n",
      "current depth: 6, current n_estimator: 26, current accuracy: 0.9890379685012236\n",
      "current depth: 6, current n_estimator: 30, current accuracy: 0.9877304045974689\n",
      "current depth: 6, current n_estimator: 33, current accuracy: 0.9901440012540436\n",
      "current depth: 6, current n_estimator: 36, current accuracy: 0.9891382887487157\n",
      "current depth: 6, current n_estimator: 40, current accuracy: 0.9899431178528908\n",
      "current depth: 8, current n_estimator: 10, current accuracy: 0.9821987833963594\n",
      "current depth: 8, current n_estimator: 13, current accuracy: 0.9858190569281374\n",
      "current depth: 8, current n_estimator: 16, current accuracy: 0.9854174520632772\n",
      "current depth: 8, current n_estimator: 20, current accuracy: 0.9879311260611761\n",
      "current depth: 8, current n_estimator: 23, current accuracy: 0.9895405413633578\n",
      "current depth: 8, current n_estimator: 26, current accuracy: 0.9884341847356468\n",
      "current depth: 8, current n_estimator: 30, current accuracy: 0.9879313689673443\n",
      "current depth: 8, current n_estimator: 33, current accuracy: 0.988132576243388\n",
      "current depth: 8, current n_estimator: 36, current accuracy: 0.9898419879181711\n",
      "current depth: 8, current n_estimator: 40, current accuracy: 0.9893394960247596\n",
      "current depth: 10, current n_estimator: 10, current accuracy: 0.9838082796672638\n",
      "current depth: 10, current n_estimator: 13, current accuracy: 0.985316565034726\n",
      "current depth: 10, current n_estimator: 16, current accuracy: 0.987126944706783\n",
      "current depth: 10, current n_estimator: 20, current accuracy: 0.9873283139202724\n",
      "current depth: 10, current n_estimator: 23, current accuracy: 0.9865234848160973\n",
      "current depth: 10, current n_estimator: 26, current accuracy: 0.9873277471392131\n",
      "current depth: 10, current n_estimator: 30, current accuracy: 0.9873281519828268\n",
      "current depth: 10, current n_estimator: 33, current accuracy: 0.9868253362145243\n",
      "current depth: 10, current n_estimator: 36, current accuracy: 0.9884341847356468\n",
      "current depth: 10, current n_estimator: 40, current accuracy: 0.9885346669205847\n",
      "Cross validation accuracy for chosen best depth 6, best number of estimator 33: 0.990144\n"
     ]
    }
   ],
   "source": [
    "# Workspace 4.12a\n",
    "#BEGIN\n",
    "# TODO: Report accuracy and other appropriate metrics\n",
    "# Make sure that the label values are >=0 if using np.eye for one-hot encoding\n",
    "\n",
    "data.y_train = np.array([1 if y == 1 else 0 for y in data.y_train])\n",
    "data.y_valid = np.array([1 if y == 1 else 0 for y in data.y_valid])\n",
    "\n",
    "depths = np.linspace(3, 10, 5, dtype = int)\n",
    "n_estimators = np.linspace(10, 40, 10, dtype = int)\n",
    "\n",
    "k = 8\n",
    "\n",
    "best_depth = -1\n",
    "best_n_estimator = -1 \n",
    "best_accuracy = 0\n",
    "\n",
    "for depth in depths:\n",
    "    for n_estimator in n_estimators:        \n",
    "        classifier = AdaBoost(n_estimators=n_estimator, base=DecisionTreeClassifier(max_depth=depth))\n",
    "        accuracy = k_fold_cv(classifier, k, data.X_train, data.y_train)\n",
    "        print(\"current depth: {}, current n_estimator: {}, current accuracy: {}\".format(depth, n_estimator, accuracy))\n",
    "\n",
    "        if accuracy > best_accuracy:\n",
    "            best_accuracy = accuracy\n",
    "            best_depth = depth        \n",
    "            best_n_estimator = n_estimator\n",
    "\n",
    "print(\"Cross validation accuracy for chosen best depth %d, best number of estimator %d: %f\" % (best_depth, best_n_estimator, best_accuracy))    \n",
    "#END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8c1d8b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on validation set with the best metrics: 0.990144\n"
     ]
    }
   ],
   "source": [
    "best_n_estimator=33\n",
    "best_depth=6\n",
    "\n",
    "classifier = AdaBoost(n_estimators=best_n_estimator, base=DecisionTreeClassifier(max_depth=best_depth))\n",
    "classifier.fit(data.X_train, data.y_train)\n",
    "accuracy = classifier.score(data.X_valid, data.y_valid)\n",
    "print(\"Accuracy on validation set with the best metrics: %f\" % (best_accuracy))    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212f5a65",
   "metadata": {},
   "source": [
    "4.12 b [1 point] What are your observations with respect to `max_depth` in the base model? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5caa826",
   "metadata": {},
   "source": [
    "% Write-up for 4.12.b <br>\n",
    "#BEGIN <br>\n",
    "\n",
    "Since AdaBoost is designed to improve the performance of weak learners that are slightly better than 50 % accuracy, max_depth should not be large to avoid overfitting the training data. As max depth increases from a small to a large number, performance increases until a certain point and then begins to decrease. AdaBoost performs better when it has the optimal max_depth value for weak learners. \n",
    "\n",
    "#END<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d7d022",
   "metadata": {},
   "source": [
    "4.12 c [1 point] What are your observations with respect to `n_estimators`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdb860a",
   "metadata": {},
   "source": [
    "% Write-up for 4.12.c <br>\n",
    "#BEGIN <br>\n",
    "Increasing the number of estimators improves AdaBoost performance because each new estimator receives weights from samples misclassified by previous estimators. Increasing the number of estimators beyond a certain point, however, results in overfitting and no significant improvement in performance.\n",
    "\n",
    "#END<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26233356",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
